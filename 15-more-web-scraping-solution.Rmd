---
title: "More Web Scraping"
author: "Bastola"
date: "`r format(Sys.Date(), ' %B %d %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      collapse = TRUE, 
                      comment=NA, 
                      warning = FALSE,
                      message = FALSE,
                      fig.height = 4, fig.width = 6, fig.align='center')

library(tidyverse) 
library(lubridate)
library(stringr)
library(rvest)
library(ggplot2movies)
library(plotly)
library(polite)
```


## Your Turn 1

### a. Scrape the [Minnesota covid incidence data table](https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/state/minnesota).

```{r}
table_covid_cases <- bow(url = "https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/state/minnesota") %>% scrape() %>%
  html_elements(css = "table") %>%
  html_table()
```

### b. What are the top 10 counties in Minnesota by the number of COVID cases?

```{r}
table_covid_cases[[2]] %>% 
  mutate(Cases = as.numeric(str_remove(Cases, "\\,"))) %>%  
  select(County, Cases) %>% 
  slice_max(Cases, n= 10)
```


## Your Turn 2

1. Go to the [the numbers webpage](https://www.the-numbers.com/movie/budgets/all) and extract the table on the front page.

```{r}
session1 <- bow(url = "https://www.the-numbers.com/movie/budgets/all") %>% scrape() %>%
  html_elements(css = "table") %>%
  html_table()

table_base <- session1[[1]]
```


2. Find out the number of pages that contain the movie table, while looking for the changes in the url in the address bar. How does the url changes when you go to the next page?

*Answer:* The starting count of the movie get concatenated to the url in increments of 100.

3. Write a for loop to store all the data in multiple pages to a single data frame.

```{r}
new_urls <- "https://www.the-numbers.com/movie/budgets/all/"

# creating two empty data-frames

table_new <-data.frame()
df2 <- data.frame()


idx <-  seq(1, 6301, 100)

for (i in 1:length(idx)) {
  new_webpage <- read_html(str_glue(new_urls, {idx[i]}))
  table_new <- html_table(new_webpage)[[1]] %>% 
  tibble::as_tibble(.name_repair = "unique") 
  df2 <- rbind(df2, table_new)
}
```


```{r, eval=FALSE}
# Alternate using bow, takes very long
df3 <- data.frame()

urls <- map(idx, function(i) str_glue("https://www.the-numbers.com/movie/budgets/all/", {i}))
sessions <- lapply(1:length(urls), function(i) bow(urls[[i]]) %>% scrape() %>% html_elements(css = "table") %>% html_table())
data <- do.call(rbind, lapply(1:64, function(i) sessions[[i]][[1]]))
```


4. Display the data table using `knitr::kable(data, format = "html")`

```{r}
knitr::kable(head(df2), format = "html")
```


5. Store the data in a `.csv` file with `write_csv(data_object, "/path/name.csv")`

```{r}
readr::write_csv(df2, "my_data.csv")
```


## Your Turn 3

```{r}
tweets <- read_csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/TrumpTweetData.csv")
w <- ggplot(data=drop_na(tweets, source), 
            aes(x=nWords, y=nRealWords, color=source)) + geom_jitter() 
ggplotly(w) 
```


## Your Turn 4

```{r}
mca <- data.frame(percent = c(23.5, 34.7, 19.8, 22.0, 27.6, 32.1, 20.2, 20.2, 26.0, 30.7, 22.1, 21.1), portfolio = factor(rep(c("Commodities", "Stocks", "Real State", "Other Assets"), 3), levels=c("Commodities", "Stocks", "Real State", "Other Assets")), year = rep(c("2019","2020","2021"), each=4))

glimpse(mca)

```



```{r}
mybar <- ggplot(mca, aes(x=year, y=percent, fill=portfolio)) + 
  labs(title="Investment Portfolio") +
  geom_bar(stat = "identity", position = "dodge") +
 # theme(legend.position = "bottom") + 
  scale_fill_viridis_d()
ggplotly(mybar) 

```



